---
title: "5-Day Challenge: Data Cleaning in R"
author: "Simon Tran"
format: docx
editor: visual
---

Data cleaning is a key part of data science, but it can be deeply frustrating. What are you supposed to do with the .json file you've been sent? How can you handle all these missing values in your data? Is there a fast way to get rid of the duplicate entries in your dataset? In this challenge, I'll tackle some of the common problems that I need to take care of before I can get started with my analysis.

Here\'s what I'll be covering in this 5-Day Challenge:

-   Reading in common data file formats: .json, .txt and .xlsx

-   Filling in missing values

-   Identifying & handling outliers

-   Removing duplicate records\
    \*Cleaning numbers (percentages, money, dates and times)

```{r}
#| output: false
library(tidyverse)
library(readxl) # for reading in xl files
library(jsonlite) # for reading in json
```

## **Day 1: Reading in Different File Types and Understanding Their Structure**

Let's first read in a JSON data

```{r}
house <- read_json('house_3.json')
```

The first dataset I'll be working on is from **Tbilisi Housing Challenge 2020** on kaggle. The data was extracted from a popular Georgian house retail site on November 12th of 2020. The JSON file represents the 'raw' unclean data, straight from over 50k html pages.

Next, let's take a look at the data in depth

```{r}
head(house,3)
```

```{r}
json_structure <- capture.output(str(house))

print(json_structure[1:16])
```
