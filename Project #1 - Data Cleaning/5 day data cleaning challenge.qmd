---
title: "5-Day Challenge: Data Cleaning in R"
author: "Simon Tran"
format: docx
editor: visual
---

Data cleaning is a key part of data science, but it can be deeply frustrating. What are you supposed to do with the .json file you've been sent? How can you handle all these missing values in your data? Is there a fast way to get rid of the duplicate entries in your dataset? In this challenge, I'll tackle some of the common problems that I need to take care of before I can get started with my analysis.

Here's what I'll be covering in this 5-Day Challenge:

-   Reading in common data file formats: .json and .xlsx

-   Filling in missing values

-   Identifying & handling outliers

-   Removing duplicate records\
    \*Cleaning numbers (percentages, money, dates and times)

For each day of this challenge, I'll be using different datasets to perform these tasks.

## **Day 1: Reading in Different File Types and Understanding Their Structure**

```{r}
#| output: false
library(tidyverse)
library(readxl) # for reading in xl files
library(jsonlite) # for reading in json
```

Let's first read in a **JSON file**

```{r}
house <- read_json('house_3.json')
```

The first dataset I'll be working on is from **Tbilisi Housing Challenge 2020** on kaggle. The data was extracted from a popular Georgian house retail site on November 12th of 2020. The JSON file represents the 'raw' unclean data, straight from over 50k html pages.

Next, let's take a look at the data structure

```{r}
head(house,3)
```

```{r}
json_structure <- capture.output(str(house))

print(json_structure[1:16])
```

```{r}
# you can pull out individiual entries using double bracket notation
# JSON is essentially lists inside a list

house[[3]][[1]] # 3rd home and address row

house[[12]][[8]] # 12th home and price row

house[[450]][[8]] # 450th home and price row
```

From the example outputs above, "Queen Ketevan Avenue, Isani, Isani District, Tbilisi" is the address of the 3rd home. "70,000" represents the price of the 12th home. Similarly, "145,000" represents the price of the 450th home. So, that's all for understanding the structure of a JSON dataset.

Next, let's read in a new file type: **.XLSX file**

```{r}
library(rio) # package that can import all sheets with one line of code
```

```{r}
snap19 <- import_list('FY19.xls')
```

This dataset covers the US Supplemental Nutrition Assistance Program, more commonly known as SNAP. The program is the successor to the Food Stamps program previously in place. The program provides food assistance to low-income families in the form of a debit card. The US Dept of Agriculture, which maintains consumption data, does not release raw data on what foods are consumed - only summary reports. NERO, MARO, SERO, etc., are all different regions in the US. For example, Arizona belongs in the WRO (West Regional Office), and New York belongs in NERO (Northeast Regional Office).

## **Day 2: Dealing with Missing Values**

```{r}
library(tidyverse)
library(mice) # package for categorical & numeric imputation
library(dplyr)
```

Let's read in the data:

```{r}
train <- read.csv('train (1).csv')
```

I will be using the train.csv file from the titanic to deal with the missing values.

```{r}
head(train,3)
```

```{r}
train <- tibble(train) # tibble() preserve all the variable types

head(train)
```

Tibble have a cleaner print format, making it easier to view and understand the data. Tibble also have a stricter syntax, which helps prevent common data manipulation errors

```{r}
str(train)
```

Let's check for missing values from the dataset

```{r}
any(is.na(train))
```

Since there are missing values in this dataset. Is the data missing at random (MAR) or is it Meaningfully Missing (MM)?

```{r}
# make a missing map!
library(Amelia)
```

```{r}
missmap(train)
```

It looks like the 2% of the missing data comes from 'Age'. The age of some passengers was simply not recorded or was lost over time. We could remove this column. However, if the column with missing values is important for the analysis and for the main question we're interested in, then removing the column may not be the right move. But let's practice removing it for this example.

```{r}
train_removed = subset(train, select = -c(Age))

head(train_removed)
```

```{r}
missmap(train_removed)
```

You can see that there are no more missing values. That's one way to deal with missing values. Again, if 'Age' was important in our analysis then this method would not be preferable. The next method that may be helpful would be imputation.

I'll use the "Mice Algorithm" (Multiple Imputation by Chained Equations), which is a statistical technique used to impute missing data in a dataset. It is based on the idea that missing values can be imputed by predicting them from other variables in the dataset.

```{r}
imp <- mice(train, m = 5, method = "pmm") # pmm means predictive mean matching
summary(imp)
```

```{r}
completedData <- complete(imp,1)
```

The missing values have been replaced with the imputed values in the first of the five datasets. This is not 100% completed. I think there might be other ways to handle missing values, but this just for practice.

## **Day 3: Identifying and Handling Outliers**

```{r}
library(outliers)
library(ggplot2)
```

Let's read in the data:

```{r}
tx_salary <- read.csv('texas_salaries.csv')
```

Database of compensation for Texas state employees, as published by [The Texas Tribune](https://salaries.texastribune.org/)

```{r}
head(tx_salary,5)
```

Next, let's plot it to get an idea of the outliers:

```{r}
ggplot(tx_salary,aes(x="ANNUAL", y=ANNUAL)) + geom_boxplot() # plotting annual salary
```

```{r}
# salary outliers within gender
ggplot(tx_salary,aes(GENDER,ANNUAL)) + geom_boxplot() + coord_flip()
```

```{r}
# salary outliers by ethnicity
ggplot(tx_salary,aes(ETHNICITY,ANNUAL)) + geom_boxplot() + coord_flip()
```

Interestingly, while mean salaries are rather similar across groups, and all groups have outliers, the most extreme outliers on the high end are white and male. The white group does not have outliers on the low end, whereas black, hispanic and other all do.

```{r}
# hrs per week and annual salary should be highly correlated and predictable.
# we do want to know if there are observations with low hours but high salaries
# or with high hour but low salaries.
# Are there people with comparable salaries but very different hours worked?
ggplot(tx_salary,aes(HRS.PER.WK, ANNUAL)) + geom_point()
```

```{r}
# as expected, a majority are 40 hrs per week
# but there are many observations of cases where people are working 40 hrs
# and making similar annual salaries to others working 20-35 hours.
# let's look at hrly rate

ggplot(tx_salary,aes(HRLY.RATE, ANNUAL)) + geom_point()
```

Looks like a lot of missing data there - it would not make sense for people with hourly rates of 0 to also have an annual salary, so hourly rate is likely not a good variable to use in this dataset.

Let's identify which rows contain outliers. We can see them in the visualizations\... but where are they in the data?

```{r}
# calculate the z-scores
outlier_scores <- scores(tx_salary$ANNUAL)
```

```{r}
# create a vector that holds TRUE if outlier_score is greater than 
# 3 or less than -3
is_outlier <- outlier_scores > 3 | outlier_scores < -3
```

```{r}
# add a column to data to indicate which are outliers on the var selected
tx_salary$annual_outlier <- is_outlier
```

Now outlier and non-outlier values can be graphed or analyzed separately

```{r}
ggplot(tx_salary,aes(ETHNICITY,ANNUAL)) + geom_boxplot() + coord_flip() +
facet_wrap(~annual_outlier)
```

```{r}
# create a dataframe with only the outliers
tx_outliers <- tx_salary[outlier_scores > 3 | outlier_scores < -3, ]

# call it
head(tx_outliers)
```

```{r}
ggplot(tx_outliers,aes(x="ANNUAL", y=ANNUAL)) + geom_boxplot()
```

Outliers from the outliers.

```{r}
ggplot(tx_outliers,aes(GENDER,ANNUAL)) + geom_boxplot() + coord_flip()
```

```{r}
ggplot(tx_outliers,aes(ETHNICITY,ANNUAL)) + geom_boxplot() + coord_flip()
```

There are both reasons for keeping and removing outliers. Sometimes, they are unnecessary noise and it is okay to get rid of them since we are trying to predict things for the masses and not for the extremes. In other cases, they are an important part of the story that the data tells. With salary data reporting, it would often be the case that removing outliers would be unrealistic and inaccurate, but we'll still practice the strats provided on this data set since we know there are outliers!

```{r}
tx_norms <- tx_salary[tx_salary$annual_outlier == FALSE, ]
```

```{r}
ggplot(tx_norms,aes(x="ANNUAL", y=ANNUAL)) + geom_boxplot()
```

```{r}
# comparing distribution of initial data to data with outliers removed
ggplot(tx_salary, aes(ANNUAL)) + geom_histogram(binwidth=10000)
```

```{r}
ggplot(tx_norms, aes(ANNUAL)) + geom_histogram(binwidth=10000)
```

Another option for dealing with outliers is removing and replacing via imputation

```{r}
# replace the outliers with NA
tx_salary[outlier_scores > 3 | outlier_scores < -3, "ANNUAL"] <- NA
```

```{r}
# check
summary(tx_salary$ANNUAL)
```

```{r}
# replace with mean (median can be more accurate in salaries,
# but they are not far off without the outliers!)

# let's use a function and then apply it to the column
impute <- function(x){
    if(is.na(x)){
        return(48400) # mean value above
    }
    else {
        return(x)
    }
}
```

```{r}
tx_salary$ANNUAL <- sapply(tx_salary$ANNUAL, FUN = impute)
```

```{r}
# check
summary(tx_salary$ANNUAL)
```

To reflect, there are many ways to deal with outliers. The common ways are removing the rows from the data, considering outliers and inliners separately, and removing and replacing via imputation.

## **Day 4: Removing Duplicate Records**

Duplicate records can be common when webscraping (the same table may exist on multiple pages, for example). Removing duplicate records is important because it can lead to incorrect conclusions about observations being higher frequency than they actually are.

For some data, another instance of the same information is important and part of the structure of the set, The stuff that gets duplicated should be unintentional repetitions that were COLLECTED or GATHERED too frequently for how often they actually occurred.

```{r}
library(tidyverse)
library(mice)
```

Let's read in the data:

```{r}
steam_data <- read.csv("steam-200k.csv", header = TRUE, stringsAsFactors = FALSE)

# add col names
names(steam_data) <- c('user_id','game_title', "behavior_name","value","x")

#remove last col and create df
steam_data <- tibble(steam_data[, 1:4])
```

Steam is the world's most popular PC Gaming hub. With a massive collection that includes everything from AAA blockbusters to small indie titles, great discovery tools can be super valuable for Steam.

This dataset is a list of user behaviors, with columns: user-id, game-title, behavior-name, value. The behaviors included are 'purchase' and 'play'. The value indicates the degree to which the behavior was performed - in the case of 'purchase' the value is always 1, and in the case of 'play' the value represents the number of hours the user has played the game.

Next, we can use the **`distinct()`** function from **`dplyr`** to identify and remove duplicate rows based on selected columns. For example, if we want to remove duplicates based on the **`user_id`** and **`game_title`** columns, we can do:

```{r}
steam_data_unique <- distinct(steam_data, user_id, game_title, .keep_all = TRUE)

```

Finally, we can write the cleaned data frame to a new CSV file:

```{r}
write.csv(steam_data_unique, "steam-200k-cleaned.csv", row.names = FALSE)
```

That was just a basic way to remove duplicate values. A good thing to note is that our original steam_data had 199999 observations, and after removing the duplicate values we're left with 128804. It's important to consider why there were duplicates in the first place and whether removing them could result in biased or incomplete data.

## **Day 5: Cleaning Numeric Columns**

Numeric data can come with all sorts of non-numeric characters such as percentage signs, number signs of other special characters that may have meaning in another programming language but act only as noise in yours.

R plays it safe - so any column that comes in with special characters is usually labeled and treated as a character column rather than a numeric column.

Regular expressions can be used, but should be used as a last resort. They are rigid, easily broken, hard to read and hard to debug. Instead, parse_number() from tidyverse can be a great option.

```{r}
library(tidyverse)
library(lubridate)
```

```{r}
# character vector of numbers
to_parse <- c(100, "10,000", "%100", "$50")

# check to make sure it's numeric
print("Class before:")
class(to_parse)

# parse numbers
parsed_numbers <- parse_number(to_parse)

# check class
print("Class after:")
class(parsed_numbers)

# see what it looks like now
parsed_numbers
```

## End of code.
